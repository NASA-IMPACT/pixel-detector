{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d4f62d",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "1. You will be working off of terminal for this chapter.\n",
    "2. Change your working directory: ```cd /p/project/training2206/$USER/```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca1130-19c1-4bc4-bfbe-a1c1d087c09d",
   "metadata": {},
   "source": [
    "# Install Monda using Miniconda\n",
    "You will be using Python and TensorFlow for training. For this you need to create an environment you can use across nodes. You will be using Miniconda to create a Python virtual environment for your experiments.\n",
    "\n",
    "## Training using high performance computing and TensorFlow\n",
    "1. Download Miniconda from https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "2. Install conda using the Miniconda script you downloaded\n",
    "3. Make sure you can use the Python from the conda environment\n",
    "4. Create a virtual environment\n",
    "5. Update environment variables\n",
    "6. Folder structure and files you will be using\n",
    "7. Update configuration for training\n",
    "8. Update batch_job file.\n",
    "9. Submit job\n",
    "10. Check progress\n",
    "11. Test saved model\n",
    "\n",
    "## Use cloud computing for inferencing\n",
    "1. Push trained model to AWS environment using boto3 (share credentials before this step)\n",
    "2. Access setup environments in SageMaker\n",
    "3. Load model in SageMaker\n",
    "4. Deploy model\n",
    "5. Test deployed model\n",
    "6. Deploy API endpoint to interact with model\n",
    "7. Interact with API endpoint to get inferences from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771ee6a-3231-4174-a1de-5881d7a5d9a2",
   "metadata": {},
   "source": [
    "## Download Miniconda from https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e44b4-2101-42e0-8260-7cb075ea306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70164300-f584-4d32-9938-3ea6efb4728e",
   "metadata": {},
   "source": [
    "## Install conda using the downloaded Miniconda shell script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18c800-4ad6-42a6-b338-b1c72a1a8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chmod 770 Miniconda3-latest-Linux-x86_64.sh\n",
    "./Miniconda3-latest-Linux-x86_64.sh\n",
    "# Where to install miniconda (after the installation, it will ask): `/p/project/training2206/$USER/miniconda3` \n",
    "# Do you wish the installer to Initialize miniconda3? Yes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783e3d5-1a5e-4a7a-98b6-c3f1574bc67d",
   "metadata": {},
   "source": [
    "### Check if the installation updated your bashrc to automatically use Python from conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c4600-e356-4613-9f2e-69a2145339f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b96b9-db5b-40fd-8422-43bbac01dda2",
   "metadata": {},
   "source": [
    "## Folder structure\n",
    "\n",
    "All of the project-related files are located at `/p/project/training2206/$USER/`. (User is the environment variable with your user name. You can check what it is set as using `echo $USER`)\n",
    "\n",
    "Change directory to the aforementioned directory.\n",
    "\n",
    "Check for `pixel-detector` folder in the directory. If it is not present, you can use Git to download it from https://github.com/nasa-impact/pixel-detector using `git clone https://github.com/nasa-impact/pixel-detector.git`\n",
    "\n",
    "Once cloned, change the directory to the pixel-detector folder using `cd pixel-detector`\n",
    "\n",
    "Below is the folder structure for the code:\n",
    "```\n",
    "|> code\n",
    "    |> lib\n",
    "        |> data_utils: `Contains files to rasterize files and create a dataset`\n",
    "        |> slurm_utils `Contains helper files for distributed training using TensorFlow`\n",
    "    |> train.py `Main file used to train the model`\n",
    "    |> models.py `Contains the architecture of the model you are training`\n",
    "    |> config.py `Configuration file`\n",
    "    |> config.json `Configuration file`\n",
    "|> data `Contains training and validation data`\n",
    "|> train_job.sh `Main batch file`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14cf13a-137d-4f5c-b2cd-610b95371b63",
   "metadata": {},
   "source": [
    "## Create virtual environment\n",
    "You will use the Python from the conda environment to create a virtual environment which will be used throughout.\n",
    "\n",
    "In some cases, conda might not be activated after installation. You can just refresh your bash terminal using `exec bash`, and it should enable the conda environment for you.\n",
    "\n",
    "Once in the conda environment, you can create a new Python virtual environment using `python -m venv .venv`\n",
    "\n",
    "Then you will use the environment you just created using `source .venv/bin/activate`\n",
    "\n",
    "Once the environment is activated, you will need to make sure you are starting from scratch. To make sure no other modules are installed, use `module purge` to remove all the unwanted modules.\n",
    "\n",
    "You can then install the requirements using `pip install -r requirements.txt`\n",
    "\n",
    "This will install all the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857057d-6d7c-4a6d-b283-875a8eee8efe",
   "metadata": {},
   "source": [
    "## Update configuration and environment variables\n",
    "\n",
    "To find your user name, run the following command in the terminal:\n",
    "\n",
    "`echo $USER`\n",
    "\n",
    "Copy data from `/p/project/training2206/sedona3/pixel-detector/data/` to your folder: \n",
    "\n",
    "`cp -r /p/project/training2206/sedona3/pixel-detector/data/ /p/project/training2206/$USER/pixel-detector/`. \n",
    "\n",
    "In `/p/project/training2206/$USER/pixel-detector/code` you will find a configuration file called `config.json`. In this file replace all instances of `<username>` with the username provided to you. \n",
    "\n",
    "Before we start working on any of this, we will change our directory to `/p/project/training2206/$USER/pixel-detector/`\n",
    "\n",
    "In the jupyter lab interface, find `code/config.json` file. Right click on `config.json` in the left pane, and select `editor`. Once the file is open, you can update the `<username>` instances with your `username`.\n",
    "\n",
    "You will also need to update the `train_job.sh` file.\n",
    "\n",
    "In the `train_job.sh` file replace all instances of `<username>` with the username provided to you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d17936-ef79-4293-9d2d-74b2a60557ea",
   "metadata": {},
   "source": [
    "# Submit Training Job\n",
    "In the `train_job.sh` you can specify the number of nodes you want to use for training. As an example, you are going to use 2 nodes for training.\n",
    "\n",
    "Check details of the training job:\n",
    "\n",
    "`cat /p/project/training2206/$USER/train_job.sh`\n",
    "\n",
    "You can submit the training job using the `sbatch` command. Like so: `sbatch train_job.sh`\n",
    "\n",
    "Once submitted, two new files will be created by the process: `output.out` and `error.err`. `output.out` will contain details of the output from your processes, and `error.err` will provide details on any errors from the scripts. Once the job is submitted and the files are created, you can check for updates simply by using `tail -f output.out error.err`. (Any warnings, automated messages, and errors are tracked in the `error.err` file while only the [ed. note: incomplete sentence]\n",
    "\n",
    "You can see how good or bad the model training is by watching the loss outputs in `output.out`. We have also prepared methods to create charts for the metrics. The training/validation, loss/accuracy plots can be found in the `plots` folder. You can scp the files using `scp .  <username>@jureca.fz-juelich.de:/p/project/training2206/<username>/pixel-detector/plots/*.png` or view it in the jupyter hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a1f02",
   "metadata": {},
   "source": [
    "# Uploading the Model to a Cloud Environment\n",
    "\n",
    "After the model is finished training, the model is stored in the location specified in your config file `/p/project/training2206/<username>/pixel-detector/models/<username>_smoke_wmts_ref_3layer.h5`. You will be taking this model and pushing it to an S3 bucket using `boto3` and the credentials from the AWS account shared with you.\n",
    "\n",
    "## Get AWS credentials\n",
    "Account creation links should have been shared with you. Once the account is setup, you can obtain the credentials required for upload from the AWS SSO homepage.\n",
    "Please follow the steps listed below:\n",
    "\n",
    "1. Navigate to https://nasa-impact.awsapps.com/start\n",
    "2. Login\n",
    "3. Click on `AWS Account`\n",
    "4. Click on `summerSchool`\n",
    "5. Click on `Command line or Programmatic access`\n",
    "6. Copy the `AWS Access Key Id`, `AWS Secret Access Key`, and `AWS session token` from the pop up\n",
    "7. Update the following script and run it in a python shell. (You can start a python shell by just typing `python` in the terminal).\n",
    "\n",
    "This will upload the files directly into the S3 bucket. You will then fetch the file from S3 bucket into the SageMaker notebook from where you will be deploying the model and hosting an API to interact with the model.\n",
    "\n",
    "\n",
    "*Note: Please make sure the virtual environment is active while working with the python shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import os\n",
    "\n",
    "AWS_ACCESS_KEY_ID = <Copied over from SSO login>\n",
    "AWS_SECRET_ACCESS_KEY = <Copied over from SSO login>\n",
    "AWS_SESSION_TOKEN = <Copied over from SSO login>\n",
    "\n",
    "BUCKET_NAME = 'smoke-dataset-bucket'\n",
    "\n",
    "USER = os.environ.get('USER')\n",
    "\n",
    "def generate_federated_session():\n",
    "    \"\"\"\n",
    "    Method to generate federated session to upload the file from HPC to S3 bucket.\n",
    "    ARGs:\n",
    "        filename: Upload filename\n",
    "    Returns: \n",
    "        Signed URL for file upload \n",
    "    \"\"\"\n",
    "    return boto3.session.Session(\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            aws_session_token=AWS_SESSION_TOKEN\n",
    "        )\n",
    "\n",
    "model_filename = f\"/p/project/training2206/{USER}/pixel-detector/models/\"\n",
    "session = generate_federated_session()\n",
    "s3_connector = session.client('s3')\n",
    "\n",
    "s3_connector.upload_file(model_filename, BUCKET_NAME, f\"{USER}/{USER}_smoke_wmts_ref_3layer.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67866e",
   "metadata": {},
   "source": [
    "Once the process is done, you can check for the files in S3 using the AWS console.\n",
    "\n",
    "1. Navigate to https://nasa-impact.awsapps.com/start\n",
    "2. Login\n",
    "3. Click on `AWS Account`\n",
    "4. Click on `summerSchool`\n",
    "5. Click on `Management Console`\n",
    "6. In the search bar, search for `s3`\n",
    "7. Click on `s3`\n",
    "8. Click on `smoke-dataset-bucket`\n",
    "9. Click on your `username`\n",
    "\n",
    "You should be able to view your file there now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7e0a2",
   "metadata": {},
   "source": [
    "# Notes on Deep Learning Model Details\n",
    "\n",
    "## Deep learning\n",
    "Deep learning refers to neural networks with multiple hidden layers that can learn increasingly abstract representations of the input data.\n",
    "\n",
    "Deep learning has led to major advances in computer vision. You’re now able to classify images, find objects in them, and even label them with captions. To do so, deep neural networks with many hidden layers can sequentially learn more complex features from the raw input image:\n",
    "* The first hidden layers might only learn local edge patterns.\n",
    "* Then, each subsequent layer (or filter) learns more complex representations.\n",
    "* Finally, the last layer can classify the image as a cat or kangaroo.\n",
    "These types of deep neural networks are called convolutional neural networks.\n",
    "\n",
    "## Convolutional neural networks\n",
    "Convolutional neural networks (CNN’s) are multi-layer neural networks (sometimes up to 17 or more layers) that assume the input data to be images.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NASA-IMPACT/workshop_notebooks/master/chapter-3/Feature_maps.png\">\n",
    "\n",
    "By making this requirement, CNN's can drastically reduce the number of parameters that need to be tuned. Therefore, CNN's can efficiently handle the high dimensionality of raw images.\n",
    "\n",
    "There are a multitude of different neural network architectures that use CNN for various tasks. For image segmentation tasks, you use U-Net model.\n",
    "\n",
    "## U-Net segmentation model\n",
    "\n",
    "The model you are using for this workshop is the U-Net segmentation model (https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/). The architecture is a stack of convolutions followed by de-convolutions that gives its U-shape.\n",
    "<img src=\"https://raw.githubusercontent.com/NASA-IMPACT/workshop_notebooks/master/chapter-3/u-net-architecture.png\">\n",
    "\n",
    "This model assigns a class label to each pixel of the input and gives an output matching the size of the input. The resulting output, once trained with smoke masks, will segment any given image into smoke and non-smoke regions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef01ad3",
   "metadata": {},
   "source": [
    "# Notes on Model Implementation\n",
    "\n",
    "## Keras: a deep learning framework\n",
    "Keras is a library for deep learning in Python. Its minimalistic, modular approach makes it easy to get deep neural networks up and running. You can read more about it here: https://keras.io/.\n",
    "\n",
    "Assuming Keras is installed in your Python environment, the main usage of the library is listed as follows.\n",
    "\n",
    "### Import Keras modules\n",
    "\n",
    "```\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "```\n",
    "### Preprocess/load images\n",
    "\n",
    "Using image libraries and preprocessing, convert images into NumPy arrays.\n",
    "\n",
    "You use the Keras sequence library (https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) to load a subset of input images in memory at a time and process in batches. This makes sure we are not running out of memory trying to load all images at once. Imgaug library (https://github.com/aleju/imgaug) is used to augment images by translations and introduce random noise to inclrease the variability in input images and help with model generalization.\n",
    "\n",
    "### Define model architecture\n",
    "\n",
    "You define a neural network model as a set of Keras layer objects, starting from input to output, and hidden layers in between. Here are some example models: https://keras.io/examples/.\n",
    "\n",
    "### Add regularization methods to prevent overfitting\n",
    "Overfitting is a case when the model learns too much of the training data and fails to replicate the performance in real-world (test) data. Regularization techniques are measures used to prevent this case.\n",
    "#### Dropouts\n",
    "This is a method for regularizing your model in order to prevent overfitting. You can read more about it here.[ed. note: no link]\n",
    "#### MaxPooling\n",
    "MaxPooling2D is a way to reduce the number of parameters in your model by sliding a 2x2 pooling filter across the previous layer and taking the max of the 4 values in the 2x2 filter.\n",
    "\n",
    "#### Batch normalization\n",
    "\n",
    "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n",
    "\n",
    "### Compile model with loss metric\n",
    "\n",
    "When you compile the model, you declare the loss function and the optimizer (e.g., SGD, Adam, etc.)\n",
    "Keras has a variety of [loss functions](https://keras.io/api/losses/) and out-of-the-box [optimizers](https://keras.io/api/optimizers/) to choose from.\n",
    "```\n",
    "Loss function dictates how far the model estimate is from the actual output (truth value), and the optimizer makes adjustments to the model variables so that the estimate is closer to the actual output.\n",
    "\n",
    "Model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=None,\n",
    "    metrics=None,\n",
    "    loss_weights=None,\n",
    "    weighted_metrics=None,\n",
    "    run_eagerly=None,\n",
    "    steps_per_execution=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "### Define callbacks\n",
    "Callbacks are pieces of code that get executed every time the model trains through one pass of all available input images. This is particularly useful functionality provided by Keras that does various tasks such as stopping training if the model does not improve significantly, saving the weights of the best model only, and plotting training graphs to better understand how the model learns in the training phase.\n",
    "\n",
    "### Training the model with the generators\n",
    " \n",
    " Finally, training the model is as simple as calling `model.fit()` method.\n",
    "\n",
    "```\n",
    "Model.fit(\n",
    "    x=None,\n",
    "    y=None,\n",
    "    batch_size=None,\n",
    "    epochs=1,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=None,\n",
    "    validation_split=0.0,\n",
    "    validation_data=None,\n",
    "    shuffle=True,\n",
    "    class_weight=None,\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    validation_batch_size=None,\n",
    "    validation_freq=1,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False,\n",
    ")\n",
    "``` \n",
    "Some notes:\n",
    "\n",
    "'steps_per_epoch' defines the number of times the generator should be called for each epoch. This number is the number of input samples (54) divided by the batch_size (4) ~= 13. Similrly, validation_step is the number of images in the validation split (38) divided by batch_size ~= 10.\n",
    "\n",
    "'epochs' just need to be sufficiently large, since we are using EarlyStopping callback to preemptively stop model training if the loss does not improve for several epochs.\n",
    "\n",
    "### Test model:\n",
    "\n",
    "Predictions can be made on any input image by calling `model.predict()` with the list of input images to predict.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
