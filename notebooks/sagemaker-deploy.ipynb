{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying model into AWS cloud environment using Sagemaker.\n",
    "\n",
    "Steps:\n",
    "- [ ] Upload model to s3 bucket\n",
    "- [ ] Convert keras to proto buf format from h5\n",
    "- [ ] Deploy an endpoint in sagemaker\n",
    "- [ ] Establish a lambda function triggered by API gateway (Code provided)\n",
    "- [ ] Notebook entry to generate s3 signed urls\n",
    "- [ ] Upload file to s3 using signed urls\n",
    "- [ ] Infer endpoint to infer on the uploaded file\n",
    "- [ ] Visualize the image and results\n",
    "- [ ] Add descriptions to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade available version of sagemaker\n",
    "\n",
    "!pip install -U sagemaker tensorflow==2.9.1 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker and tensorflow model to load the model from s3 bucket.\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow imports needed for model conversion.\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "\n",
    "from tensorflow.python.saved_model import builder as model_builder\n",
    "from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import Session\n",
    "from sagemaker.tensorflow.serving import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account related constants\n",
    "\n",
    "BUCKET = \"smoke-dataset-bucket\"\n",
    "EXECUTION_ROLE = 'arn:aws:iam::574347909231:role/igarss-sagemaker-role'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"<username>_smoke_wmts_ref_3layer.h5\" # <model filename> # smoke_wmts_ref_3layer.h5\n",
    "framework_version = \"2.9.1\" # <tensorflow version> # 2.9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "print(role)\n",
    "session = Session()\n",
    "session.download_data('.', BUCKET, key_prefix=f\"<username>/{model_filename}\", extra_args=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Keras hdf-5 format to proto-buf format\n",
    "\n",
    "We used Keras to train and save our model. This is not recognized by the Sagemaker processes as a native model format. Hence, we will need to change the format to Protocol Buffer format which is easier to be read by Sagemaker.\n",
    "\n",
    "The following methods help convert the current format to Proto-Buf format and prepare a package which can then be deployed to the Sagemaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "MODEL_VERSION = '1'\n",
    "EXPORT_DIRECTORY = 'export/Servo/{}'\n",
    "MODEL_ARCHIVE = '<name>model.tar.gz'\n",
    "\n",
    "\n",
    "def upload_model(model_archive=MODEL_ARCHIVE):\n",
    "    \"\"\"\n",
    "    Method to upload proto-buf based model to S3.\n",
    "    Args:\n",
    "        model_archive(str): name of the model archive.\n",
    "    Returns:\n",
    "        model_data: details of the model that was uploaded\n",
    "    \"\"\"\n",
    "    with tarfile.open(model_archive, mode='w:gz') as archive:\n",
    "        archive.add('export', recursive=True) \n",
    "\n",
    "    role = get_execution_role()\n",
    "    sess = Session()\n",
    "    bucket = sess.default_bucket()\n",
    "    \n",
    "    # upload model artifacts to S3\n",
    "    return sess.upload_data(\n",
    "            path=model_archive, \n",
    "            key_prefix='model'\n",
    "        )\n",
    "\n",
    "\n",
    "def convert_to_proto_buf(model_name):\n",
    "    \"\"\"\n",
    "    Converts Keras based model to Protocol Buffer format recognized by Tensorflow\n",
    "    Args:\n",
    "        model_name: File path to the model we want to convert.\n",
    "    Return: \n",
    "        signature: Input/Output signature of the loaded model\n",
    "    \"\"\"\n",
    "    loaded_model = load_model(model_name)\n",
    "    export_dir =  EXPORT_DIRECTORY.format(MODEL_VERSION)\n",
    "    builder = model_builder.SavedModelBuilder(export_dir)\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    signature = predict_signature_def(\n",
    "            inputs={\"inputs\": loaded_model.input}, outputs={\"score\": loaded_model.output}\n",
    "        )\n",
    "    builder.add_meta_graph_and_variables(\n",
    "    sess=K.get_session(), tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n",
    "    builder.save()\n",
    "    return signature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTANCE_TYPE = 'ml.t2.medium'\n",
    "\n",
    "convert_to_proto_buf(model_filename)\n",
    "model_data = upload_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Sagemaker model based on uploaded model\n",
    "\n",
    "Once the model is uploaded, we need to let Sagemaker know where to get the model and provide an execution role which will have access to the S3 bucket where the model was pushed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_model = TensorFlowModel(\n",
    "        model_data=model_data, \n",
    "        framework_version='2.8',\n",
    "        role=EXECUTION_ROLE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "Now, we deploy the model into an endpoint. Endpoints are how models are hosted. Once the endpoints are established, we can add a lambda function which interacts with the endpoint and use it as the API backend.\n",
    "\n",
    "Here, we are using `ml.t2.medium` instance to host our sagemaker endpoint. If there is a higher memory requirement, we can change the type of instance depending on our needs.\n",
    "\n",
    "_Note: Change `<name>` to reflect your name._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "uncompiled_predictor = sagemaker_model.deploy(\n",
    "        initial_instance_count=1, \n",
    "        instance_type=INSTANCE_TYPE,\n",
    "        endpoint_name=\"<name>-prediction-endpoint\"\n",
    "    )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the local and deployed version of the model.\n",
    "\n",
    "The endpoint should be available for us to use in a couple of minutes. Once the model is deployed, we can compare the inferences betwee Keras version and Proto-buf deployed version.\n",
    "\n",
    "Here, we are just creating random values and infering on the data point using both versions of the model. The difference between the inferences should be negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally load the model for comparison\n",
    "loaded_model = load_model(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.randn(1, 256, 256, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_preds = uncompiled_predictor.predict(data)\n",
    "original_model_preds = loaded_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = deployed_model_preds['predictions'] - original_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
